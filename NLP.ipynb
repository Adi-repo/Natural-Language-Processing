{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       Just got sent this photo from Ruby #Alaska as ...\n",
       "                              ...                        \n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @TheTawniest The out of control w...\n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will extract all the unique words from the dataset\n",
    "def unique_words(df):\n",
    "    unique_words = []\n",
    "    for i in range(len(df)):\n",
    "        for word in df['text'][i].split():\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "    return unique_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31924"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will create a dictionary of all the unique words and their frequency in the dataset\n",
    "def word_freq(df):\n",
    "    unique_words = []\n",
    "    for i in range(len(df)):\n",
    "        for word in df['text'][i].split():\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "    word_freq = {}\n",
    "    for word in unique_words:\n",
    "        word_freq[word] = 0\n",
    "    for i in range(len(df)):\n",
    "        for word in df['text'][i].split():\n",
    "            word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = word_freq(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do tokenization of all the words present in the dictionary\n",
    "def tokenize_words(dictionary):\n",
    "    tokens = {}\n",
    "    ind = 0\n",
    "    for word in dictionary:\n",
    "        tokens[word] = ind\n",
    "        ind += 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_words(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "submission = pd.read_csv('dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text   \n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...  \\\n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary which has id as keys and text as values\n",
    "def create_dict(df):\n",
    "    dict = {}\n",
    "    for i in range(len(df)):\n",
    "        dict[df['id'][i]] = df['text'][i]\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = create_dict(train)\n",
    "#print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['id','keyword', 'location'], axis=1, inplace=True)\n",
    "test.drop(['id', 'keyword', 'location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to find number of zeroes and ones in the target column\n",
    "def count_zeroes_ones(target):\n",
    "    zeroes = 0\n",
    "    ones = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == 0:\n",
    "            zeroes += 1\n",
    "        else:\n",
    "            ones += 1\n",
    "    return zeroes, ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroes, ones = count_zeroes_ones(train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf+klEQVR4nO3dfXBU1eH/8c+GJEsIbBCUhJQEUIoQBGtAw4JoKQ9bG5haoAMjAyhPhQYwoEAzXw2CtDhQSnEQqXVKnFaqUgcthAdjMKHFFDAaCU9RGAq0uAlWkwWEBJLz+8PJ/bnyIAkJyQnv18ydSe499+y5mdnNeza7G5cxxggAAMAiIQ29AAAAgJoiYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYJ7ShF1BfqqqqdPLkSbVq1Uoul6uhlwMAAK6BMUanT59WbGysQkKu/DxLkw2YkydPKi4urqGXAQAAauHEiRPq0KHDFY832YBp1aqVpK9/AB6Pp4FXAwAArkUgEFBcXJzze/xKmmzAVP/ZyOPxEDAAAFjmu17+wYt4AQCAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgndCGXoCVvuNffAM3PWMaegUAmjiegQEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGCd6wqY5557Ti6XS6mpqc6+8+fPKyUlRW3btlXLli01cuRIFRcXB513/PhxJScnq0WLFmrXrp3mzp2rixcvBo3JyclRYmKi3G63unTpooyMjOtZKgAAaEJqHTB79uzRH/7wB/Xq1Sto/+zZs7Vx40atX79eubm5OnnypEaMGOEcr6ysVHJysioqKvT+++/rlVdeUUZGhtLT050xR48eVXJysgYOHKiCggKlpqZq8uTJ2rZtW22XCwAAmhJTC6dPnzbf//73TVZWlnnwwQfN448/bowxprS01ISFhZn169c7Yw8ePGgkmby8PGOMMZs3bzYhISHG7/c7Y1588UXj8XhMeXm5McaYefPmmR49egTd5ujRo43P57vmNZaVlRlJpqysrDaXeHUSGxvb1TYAqKVr/f1dq2dgUlJSlJycrMGDBwftz8/P14ULF4L2d+vWTfHx8crLy5Mk5eXlqWfPnoqOjnbG+Hw+BQIB7d+/3xnz7bl9Pp8zx+WUl5crEAgEbQAAoGkKrekJr732mj788EPt2bPnkmN+v1/h4eFq3bp10P7o6Gj5/X5nzDfjpfp49bGrjQkEAjp37pwiIiIuue0lS5Zo4cKFNb0cAABgoRo9A3PixAk9/vjjevXVV9W8efP6WlOtpKWlqayszNlOnDjR0EsCAAD1pEYBk5+fr5KSEiUmJio0NFShoaHKzc3V888/r9DQUEVHR6uiokKlpaVB5xUXFysmJkaSFBMTc8m7kqq//64xHo/nss++SJLb7ZbH4wnaAABA01SjgBk0aJAKCwtVUFDgbH369NHYsWOdr8PCwpSdne2cU1RUpOPHj8vr9UqSvF6vCgsLVVJS4ozJysqSx+NRQkKCM+abc1SPqZ4DAADc3Gr0GphWrVrprrvuCtoXGRmptm3bOvsnTZqkOXPmqE2bNvJ4PJo5c6a8Xq/69u0rSRo6dKgSEhI0btw4LV26VH6/X0899ZRSUlLkdrslSdOmTdOqVas0b948TZw4Udu3b9cbb7yhzMzMurhmAABguRq/iPe7rFixQiEhIRo5cqTKy8vl8/m0evVq53izZs20adMmTZ8+XV6vV5GRkZowYYIWLVrkjOncubMyMzM1e/ZsrVy5Uh06dNDLL78sn89X18sFAAAWchljTEMvoj4EAgFFRUWprKys7l8P43LV7XxAU9M0H1YA3ADX+vub/4UEAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALBOaEMvAAAaK9dCV0MvAWi0zALToLfPMzAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDo1CpgXX3xRvXr1ksfjkcfjkdfr1ZYtW5zj58+fV0pKitq2bauWLVtq5MiRKi4uDprj+PHjSk5OVosWLdSuXTvNnTtXFy9eDBqTk5OjxMREud1udenSRRkZGbW/QgAA0OTUKGA6dOig5557Tvn5+frggw/0ox/9SD/96U+1f/9+SdLs2bO1ceNGrV+/Xrm5uTp58qRGjBjhnF9ZWank5GRVVFTo/fff1yuvvKKMjAylp6c7Y44ePark5GQNHDhQBQUFSk1N1eTJk7Vt27Y6umQAAGA7lzHGXM8Ebdq00bJlyzRq1CjddtttWrdunUaNGiVJOnTokLp37668vDz17dtXW7Zs0bBhw3Ty5ElFR0dLktasWaP58+fr1KlTCg8P1/z585WZmal9+/Y5tzFmzBiVlpZq69at17yuQCCgqKgolZWVyePxXM8lXsrlqtv5gKbm+h5WGg3XQu7rwJWYBfVzP7/W39+1fg1MZWWlXnvtNZ09e1Zer1f5+fm6cOGCBg8e7Izp1q2b4uPjlZeXJ0nKy8tTz549nXiRJJ/Pp0Ag4DyLk5eXFzRH9ZjqOa6kvLxcgUAgaAMAAE1TjQOmsLBQLVu2lNvt1rRp07RhwwYlJCTI7/crPDxcrVu3DhofHR0tv98vSfL7/UHxUn28+tjVxgQCAZ07d+6K61qyZImioqKcLS4urqaXBgAALFHjgLnzzjtVUFCgXbt2afr06ZowYYIOHDhQH2urkbS0NJWVlTnbiRMnGnpJAACgnoTW9ITw8HB16dJFktS7d2/t2bNHK1eu1OjRo1VRUaHS0tKgZ2GKi4sVExMjSYqJidHu3buD5qt+l9I3x3z7nUvFxcXyeDyKiIi44rrcbrfcbndNLwcAAFjouj8HpqqqSuXl5erdu7fCwsKUnZ3tHCsqKtLx48fl9XolSV6vV4WFhSopKXHGZGVlyePxKCEhwRnzzTmqx1TPAQAAUKNnYNLS0vTQQw8pPj5ep0+f1rp165STk6Nt27YpKipKkyZN0pw5c9SmTRt5PB7NnDlTXq9Xffv2lSQNHTpUCQkJGjdunJYuXSq/36+nnnpKKSkpzrMn06ZN06pVqzRv3jxNnDhR27dv1xtvvKHMzMy6v3oAAGClGgVMSUmJxo8fr88++0xRUVHq1auXtm3bpiFDhkiSVqxYoZCQEI0cOVLl5eXy+XxavXq1c36zZs20adMmTZ8+XV6vV5GRkZowYYIWLVrkjOncubMyMzM1e/ZsrVy5Uh06dNDLL78sn89XR5cMAABsd92fA9NY8TkwQANqIg8rfA4McGXWfg4MAABAQyFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHVqFDBLlizRvffeq1atWqldu3Z6+OGHVVRUFDTm/PnzSklJUdu2bdWyZUuNHDlSxcXFQWOOHz+u5ORktWjRQu3atdPcuXN18eLFoDE5OTlKTEyU2+1Wly5dlJGRUbsrBAAATU6NAiY3N1cpKSn617/+paysLF24cEFDhw7V2bNnnTGzZ8/Wxo0btX79euXm5urkyZMaMWKEc7yyslLJycmqqKjQ+++/r1deeUUZGRlKT093xhw9elTJyckaOHCgCgoKlJqaqsmTJ2vbtm11cMkAAMB2LmOMqe3Jp06dUrt27ZSbm6sHHnhAZWVluu2227Ru3TqNGjVKknTo0CF1795deXl56tu3r7Zs2aJhw4bp5MmTio6OliStWbNG8+fP16lTpxQeHq758+crMzNT+/btc25rzJgxKi0t1datW69pbYFAQFFRUSorK5PH46ntJV6ey1W38wFNTe0fVhoV10Lu68CVmAX1cz+/1t/f1/UamLKyMklSmzZtJEn5+fm6cOGCBg8e7Izp1q2b4uPjlZeXJ0nKy8tTz549nXiRJJ/Pp0AgoP379ztjvjlH9ZjqOS6nvLxcgUAgaAMAAE1TrQOmqqpKqamp6t+/v+666y5Jkt/vV3h4uFq3bh00Njo6Wn6/3xnzzXipPl597GpjAoGAzp07d9n1LFmyRFFRUc4WFxdX20sDAACNXK0DJiUlRfv27dNrr71Wl+uptbS0NJWVlTnbiRMnGnpJAACgnoTW5qQZM2Zo06ZN2rFjhzp06ODsj4mJUUVFhUpLS4OehSkuLlZMTIwzZvfu3UHzVb9L6Ztjvv3OpeLiYnk8HkVERFx2TW63W263uzaXAwAALFOjZ2CMMZoxY4Y2bNig7du3q3PnzkHHe/furbCwMGVnZzv7ioqKdPz4cXm9XkmS1+tVYWGhSkpKnDFZWVnyeDxKSEhwxnxzjuox1XMAAICbW42egUlJSdG6dev09ttvq1WrVs5rVqKiohQREaGoqChNmjRJc+bMUZs2beTxeDRz5kx5vV717dtXkjR06FAlJCRo3LhxWrp0qfx+v5566imlpKQ4z6BMmzZNq1at0rx58zRx4kRt375db7zxhjIzM+v48gEAgI1q9DZq1xXePrx27Vo9+uijkr7+ILsnnnhCf/3rX1VeXi6fz6fVq1c7fx6SpGPHjmn69OnKyclRZGSkJkyYoOeee06hof+/p3JycjR79mwdOHBAHTp00NNPP+3cxrXgbdRAA+Jt1ECT19Bvo76uz4FpzAgYoAE1kYcVAga4soYOGP4XEgAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxT44DZsWOHhg8frtjYWLlcLr311ltBx40xSk9PV/v27RUREaHBgwfr008/DRrzxRdfaOzYsfJ4PGrdurUmTZqkM2fOBI3Zu3evBgwYoObNmysuLk5Lly6t+dUBAIAmqcYBc/bsWd1999164YUXLnt86dKlev7557VmzRrt2rVLkZGR8vl8On/+vDNm7Nix2r9/v7KysrRp0ybt2LFDU6dOdY4HAgENHTpUHTt2VH5+vpYtW6ZnnnlGL730Ui0uEQAANDUuY4yp9ckulzZs2KCHH35Y0tfPvsTGxuqJJ57Qk08+KUkqKytTdHS0MjIyNGbMGB08eFAJCQnas2eP+vTpI0naunWrfvKTn+g///mPYmNj9eKLL+r//u//5Pf7FR4eLkn61a9+pbfeekuHDh26prUFAgFFRUWprKxMHo+ntpd4pQuv2/mApqb2DyuNimsh93XgSsyC+rmfX+vv7zp9DczRo0fl9/s1ePBgZ19UVJSSkpKUl5cnScrLy1Pr1q2deJGkwYMHKyQkRLt27XLGPPDAA068SJLP51NRUZG+/PLLy952eXm5AoFA0AYAAJqmOg0Yv98vSYqOjg7aHx0d7Rzz+/1q165d0PHQ0FC1adMmaMzl5vjmbXzbkiVLFBUV5WxxcXHXf0EAAKBRajLvQkpLS1NZWZmznThxoqGXBAAA6kmdBkxMTIwkqbi4OGh/cXGxcywmJkYlJSVBxy9evKgvvvgiaMzl5vjmbXyb2+2Wx+MJ2gAAQNNUpwHTuXNnxcTEKDs729kXCAS0a9cueb1eSZLX61Vpaany8/OdMdu3b1dVVZWSkpKcMTt27NCFCxecMVlZWbrzzjt1yy231OWSAQCAhWocMGfOnFFBQYEKCgokff3C3YKCAh0/flwul0upqalavHix/v73v6uwsFDjx49XbGys806l7t2768c//rGmTJmi3bt3a+fOnZoxY4bGjBmj2NhYSdIjjzyi8PBwTZo0Sfv379frr7+ulStXas6cOXV24QAAwF6hNT3hgw8+0MCBA53vq6NiwoQJysjI0Lx583T27FlNnTpVpaWluv/++7V161Y1b97cOefVV1/VjBkzNGjQIIWEhGjkyJF6/vnnneNRUVF65513lJKSot69e+vWW29Venp60GfFAACAm9d1fQ5MY8bnwAANqIk8rPA5MMCVNanPgQEAALgRCBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgHQIGAABYh4ABAADWIWAAAIB1CBgAAGAdAgYAAFiHgAEAANYhYAAAgHUIGAAAYB0CBgAAWIeAAQAA1iFgAACAdQgYAABgnUYdMC+88II6deqk5s2bKykpSbt3727oJQEAgEag0QbM66+/rjlz5mjBggX68MMPdffdd8vn86mkpKShlwYAABpYow2Y3/3ud5oyZYoee+wxJSQkaM2aNWrRooX+9Kc/NfTSAABAAwtt6AVcTkVFhfLz85WWlubsCwkJ0eDBg5WXl3fZc8rLy1VeXu58X1ZWJkkKBAL1u1gAl2oq97vzDb0AoPGqr9+v1fMaY646rlEGzOeff67KykpFR0cH7Y+OjtahQ4cue86SJUu0cOHCS/bHxcXVyxoBXEVUVEOvAEA9i3qufu/np0+fVtRVHksaZcDURlpamubMmeN8X1VVpS+++EJt27aVy+VqwJWhvgUCAcXFxenEiRPyeDwNvRwA9YD7+c3DGKPTp08rNjb2quMaZcDceuutatasmYqLi4P2FxcXKyYm5rLnuN1uud3uoH2tW7euryWiEfJ4PDywAU0c9/Obw9WeeanWKF/EGx4ert69eys7O9vZV1VVpezsbHm93gZcGQAAaAwa5TMwkjRnzhxNmDBBffr00X333aff//73Onv2rB577LGGXhoAAGhgjTZgRo8erVOnTik9PV1+v18/+MEPtHXr1kte2Au43W4tWLDgkj8hAmg6uJ/j21zmu96nBAAA0Mg0ytfAAAAAXA0BAwAArEPAAAAA6xAwAADAOgQMAACwDgEDa1VUVDT0EgAADYSAwQ3z73//Wy6X65Lthz/8oSTpn//8pwYMGKCIiAjFxcVp1qxZOnv2rHN+p06d9Oyzz2r8+PHyeDyaOnWqJOnNN99Ujx495Ha71alTJy1fvjzodsvLy/Xkk0/qe9/7niIjI5WUlKScnBzn+LFjxzR8+HDdcsstioyMVI8ePbR58+Z6/3kAN6Py8nLNmjVL7dq1U/PmzXX//fdrz549kqScnBy5XC5lZ2erT58+atGihfr166eioqKgOd5++20lJiaqefPmuv3227Vw4UJdvHhR0tf/R+eZZ55RfHy83G63YmNjNWvWrBt+nbgBDHCDXLx40Xz22WfO9tFHH5m2bduap59+2hw+fNhERkaaFStWmE8++cTs3LnT3HPPPebRRx91zu/YsaPxeDzmt7/9rTl8+LA5fPiw+eCDD0xISIhZtGiRKSoqMmvXrjURERFm7dq1znmTJ082/fr1Mzt27DCHDx82y5YtM26323zyySfGGGOSk5PNkCFDzN69e82RI0fMxo0bTW5u7o3+8QA3hVmzZpnY2FizefNms3//fjNhwgRzyy23mP/973/mvffeM5JMUlKSycnJMfv37zcDBgww/fr1c87fsWOH8Xg8JiMjwxw5csS88847plOnTuaZZ54xxhizfv164/F4zObNm82xY8fMrl27zEsvvdRQl4t6RMCgQZw7d84kJSWZYcOGmcrKSjNp0iQzderUoDH/+Mc/TEhIiDl37pwx5uuAefjhh4PGPPLII2bIkCFB++bOnWsSEhKMMcYcO3bMNGvWzPz3v/8NGjNo0CCTlpZmjDGmZ8+ezoMfgPpz5swZExYWZl599VVnX0VFhYmNjTVLly51Aubdd991jmdmZhpJzuPAoEGDzG9+85ugef/85z+b9u3bG2OMWb58uenataupqKi4AVeEhsSfkNAgJk6cqNOnT2vdunUKCQnRxx9/rIyMDLVs2dLZfD6fqqqqdPToUee8Pn36BM1z8OBB9e/fP2hf//799emnn6qyslKFhYWqrKxU165dg+bOzc3VkSNHJEmzZs3S4sWL1b9/fy1YsEB79+6t/x8AcBM6cuSILly4EHSfDQsL03333aeDBw86+3r16uV83b59e0lSSUmJJOnjjz/WokWLgu7PU6ZM0WeffaavvvpKP//5z3Xu3DndfvvtmjJlijZs2OD8eQlNS6P9X0houhYvXqxt27Zp9+7datWqlSTpzJkz+sUvfnHZv1XHx8c7X0dGRtbots6cOaNmzZopPz9fzZo1CzrWsmVLSdLkyZPl8/mUmZmpd955R0uWLNHy5cs1c+bMml4agDoQFhbmfO1yuSRJVVVVkr6+Ty9cuFAjRoy45LzmzZsrLi5ORUVFevfdd5WVlaVf/vKXWrZsmXJzc4Pmhf0IGNxQb775phYtWqQtW7bojjvucPYnJibqwIED6tKlS43m6969u3bu3Bm0b+fOneratauaNWume+65R5WVlSopKdGAAQOuOE9cXJymTZumadOmKS0tTX/84x8JGKCO3XHHHQoPD9fOnTvVsWNHSdKFCxe0Z88epaamXtMciYmJKioquupjRUREhIYPH67hw4crJSVF3bp1U2FhoRITE+viMtBIEDC4Yfbt26fx48dr/vz56tGjh/x+vyQpPDxc8+fPV9++fTVjxgxNnjxZkZGROnDggLKysrRq1aorzvnEE0/o3nvv1bPPPqvRo0crLy9Pq1at0urVqyVJXbt21dixYzV+/HgtX75c99xzj06dOqXs7Gz16tVLycnJSk1N1UMPPaSuXbvqyy+/1Hvvvafu3bvfkJ8JcDOJjIzU9OnTNXfuXLVp00bx8fFaunSpvvrqK02aNEkff/zxd86Rnp6uYcOGKT4+XqNGjXL+BL1v3z4tXrxYGRkZqqysVFJSklq0aKG//OUvioiIcIIJTUhDvwgHN4+1a9caSZdsDz74oDHGmN27d5shQ4aYli1bmsjISNOrVy/z61//2jm/Y8eOZsWKFZfM+7e//c0kJCSYsLAwEx8fb5YtWxZ0vKKiwqSnp5tOnTqZsLAw0759e/Ozn/3M7N271xhjzIwZM8wdd9xh3G63ue2228y4cePM559/Xm8/B+Bmdu7cOTNz5kxz6623Grfbbfr37292795tjDHOi3i//PJLZ/xHH31kJJmjR486+7Zu3Wr69etnIiIijMfjMffdd5/zTqMNGzaYpKQk4/F4TGRkpOnbt2/Qi4LRdLiMMaYB+wkAAKDGeBcSAACwDgEDAACsQ8AAAADrEDAAAMA6BAwAALAOAQMAAKxDwAAAAOsQMAAAwDoEDAAAsA4BAwAArEPAAAAA6xAwAADAOv8PNJywP4EZ/gAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot a bar graph to show the distribution of the target column\n",
    "import matplotlib.pyplot as plt\n",
    "# change color of the bars\n",
    "plt.bar(['zeroes', 'ones'], [zeroes, ones], color=['red', 'green'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lablels = train['target'].tolist()\n",
    "tweets = train['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(tweets, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class twitterdataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = twitterdataset(inputs, lablels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [01:30<00:00, 4.89MB/s] \n",
      "d:\\Anaconda\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aditya Singh\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 6/1428 [11:53<46:31:37, 117.79s/it]"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    # eval_dataset=test_dataset            # evaluation dataset\n",
    "    # compute_metrics=compute_metrics     # define metrics function\n",
    "    # optimizers=[optimizer]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
